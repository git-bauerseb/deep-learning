{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bachelor_thesis_experimenting.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPlDzidPGSdyH0GO4wUVPGs"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dr5ZpurydK8Q"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import zipfile\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "from glob import glob\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "from tensorflow.keras.layers import Input, Conv2D, Flatten, Dense, Conv2DTranspose, Reshape, Lambda, Activation, BatchNormalization, LeakyReLU, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint \n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# Wird benötigt, da ansonsten ein 'symbolic constant' Fehler auftritt\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "\n",
        "## VARIATIONAL AUTOENCODER TEMPLATE CLASS\n",
        "class VariationalAutoencoder():\n",
        "  def __init__(\n",
        "      self,\n",
        "      input_dim,\n",
        "      encoder_conv_filters,\n",
        "      encoder_conv_kernel_size,\n",
        "      encoder_conv_strides,\n",
        "      decoder_conv_t_filters,\n",
        "      decoder_conv_t_kernel_size,\n",
        "      decoder_conv_t_strides,\n",
        "      z_dim,\n",
        "      use_batch_norm=False,\n",
        "      use_dropout=False,\n",
        "      version=1\n",
        "  ):\n",
        "\n",
        "    self.name = 'variational_autoencoder_{}'.format(version)\n",
        "    self.version=version\n",
        "    self.input_dim = input_dim\n",
        "    self.encoder_conv_filters = encoder_conv_filters\n",
        "    self.encoder_conv_kernel_size = encoder_conv_kernel_size\n",
        "    self.encoder_conv_strides = encoder_conv_strides\n",
        "    \n",
        "    self.decoder_conv_t_filters = decoder_conv_t_filters\n",
        "    self.decoder_conv_t_kernel_size = decoder_conv_t_kernel_size\n",
        "    self.decoder_conv_t_strides = decoder_conv_t_strides\n",
        "\n",
        "    self.z_dim = z_dim\n",
        "    self.use_batch_norm = use_batch_norm\n",
        "    self.use_dropout = use_dropout\n",
        "\n",
        "    self.n_layers_encoder = len(encoder_conv_filters)\n",
        "    self.n_layers_decoder = len(decoder_conv_t_filters)\n",
        "\n",
        "    self._build()\n",
        "\n",
        "  def train_with_generator(self, data_flow, epochs, lr_decay=1):\n",
        "    self.model.fit_generator(\n",
        "      data_flow,\n",
        "      shuffle=True,\n",
        "      epochs=epochs\n",
        "    )\n",
        "\n",
        "\n",
        "  def _build(self):\n",
        "    \n",
        "    ### THE ENCODER\n",
        "    encoder_input = Input(shape=self.input_dim, name='encoder_input')\n",
        "\n",
        "    x = encoder_input\n",
        "\n",
        "    for i in range(self.n_layers_encoder):\n",
        "        conv_layer = Conv2D(\n",
        "            filters = self.encoder_conv_filters[i]\n",
        "            , kernel_size = self.encoder_conv_kernel_size[i]\n",
        "            , strides = self.encoder_conv_strides[i]\n",
        "            , padding = 'same'\n",
        "            , name = 'encoder_conv_' + str(i)\n",
        "            )\n",
        "\n",
        "        x = conv_layer(x)\n",
        "\n",
        "        if self.use_batch_norm:\n",
        "            x = BatchNormalization()(x)\n",
        "\n",
        "        x = LeakyReLU()(x)\n",
        "\n",
        "        if self.use_dropout:\n",
        "            x = Dropout(rate = 0.25)(x)\n",
        "\n",
        "    shape_before_flattening = K.int_shape(x)[1:]\n",
        "\n",
        "    x = Flatten()(x)\n",
        "    self.mu = Dense(self.z_dim, name='mu')(x)\n",
        "    self.log_var = Dense(self.z_dim, name='log_var')(x)\n",
        "\n",
        "    self.encoder_mu_log_var = Model(encoder_input, (self.mu, self.log_var))\n",
        "\n",
        "    def sampling(args):\n",
        "        mu, log_var = args\n",
        "        epsilon = K.random_normal(shape=K.shape(mu), mean=0., stddev=1.)\n",
        "        return mu + K.exp(log_var / 2) * epsilon\n",
        "\n",
        "    encoder_output = Lambda(sampling, name='encoder_output')([self.mu, self.log_var])\n",
        "\n",
        "    self.encoder = Model(encoder_input, encoder_output)\n",
        "    \n",
        "    \n",
        "\n",
        "    ### THE DECODER\n",
        "\n",
        "    decoder_input = Input(shape=(self.z_dim,), name='decoder_input')\n",
        "\n",
        "    x = Dense(np.prod(shape_before_flattening))(decoder_input)\n",
        "    x = Reshape(shape_before_flattening)(x)\n",
        "\n",
        "    for i in range(self.n_layers_decoder):\n",
        "        conv_t_layer = Conv2DTranspose(\n",
        "            filters = self.decoder_conv_t_filters[i]\n",
        "            , kernel_size = self.decoder_conv_t_kernel_size[i]\n",
        "            , strides = self.decoder_conv_t_strides[i]\n",
        "            , padding = 'same'\n",
        "            , name = 'decoder_conv_t_' + str(i)\n",
        "            )\n",
        "\n",
        "        x = conv_t_layer(x)\n",
        "\n",
        "        if i < self.n_layers_decoder - 1:\n",
        "            if self.use_batch_norm:\n",
        "                x = BatchNormalization()(x)\n",
        "            x = LeakyReLU()(x)\n",
        "            if self.use_dropout:\n",
        "                x = Dropout(rate = 0.25)(x)\n",
        "        else:\n",
        "            x = Activation('sigmoid')(x)\n",
        "\n",
        "        \n",
        "\n",
        "    decoder_output = x\n",
        "\n",
        "    self.decoder = Model(decoder_input, decoder_output)\n",
        "\n",
        "    ### THE FULL VAE\n",
        "    model_input = encoder_input\n",
        "    model_output = self.decoder(encoder_output)\n",
        "\n",
        "    self.model = Model(model_input, model_output)\n",
        "\n",
        "\n",
        "  def compile(self, learning_rate, r_loss_factor):\n",
        "      def vae_r_loss(y_true, y_pred):\n",
        "        r_loss = K.mean(K.square(y_true - y_pred), axis=[1,2,3])\n",
        "        return r_loss_factor * r_loss\n",
        "\n",
        "      def vae_kl_loss(y_true, y_pred):\n",
        "        '''\n",
        "          Berechnet die Kullback-Leibler Divergenz zwischen dem\n",
        "          vorhergesagtem Wert und dem ursprünglichen Wert.\n",
        "\n",
        "          Da es sich um Gauß-Verteilungen handelt, hat die KL-Divergenz die Form:\n",
        "\n",
        "          DL(p,q) = 1/2 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "        '''\n",
        "        kl_loss = -.5 * K.sum(1 + self.log_var - K.square(self.mu) - K.exp(self.log_var), axis=1)\n",
        "        return kl_loss\n",
        "\n",
        "      def vae_loss(y_true, y_pred):\n",
        "        r_loss = vae_r_loss(y_true, y_pred)\n",
        "        kl_loss = vae_kl_loss(y_true, y_pred)\n",
        "        return r_loss + kl_loss\n",
        "\n",
        "      optimizer = Adam(learning_rate=learning_rate)\n",
        "      self.model.compile(optimizer=optimizer, loss=vae_loss, metrics=[vae_r_loss, vae_kl_loss])\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYwxCLZtdRpx",
        "outputId": "5b2f5bba-9918-4fd7-a2cb-6c9b2d716b9d"
      },
      "source": [
        "INPUT_DIMENSION = (316,316,1)\n",
        "\n",
        "# Hyperparameters\n",
        "LEARNING_RATE = 0.0001\n",
        "LOSS_FACTOR = 10000\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "vae = VariationalAutoencoder(\n",
        "  input_dim = INPUT_DIMENSION\n",
        "  , encoder_conv_filters=[32,64,64, 64]\n",
        "  , encoder_conv_kernel_size=[3,3,3,3]\n",
        "  , encoder_conv_strides=[2,2,2,2]\n",
        "  , decoder_conv_t_filters=[64,64,32,3]\n",
        "  , decoder_conv_t_kernel_size=[3,3,3,3]\n",
        "  , decoder_conv_t_strides=[2,2,2,2]\n",
        "  , z_dim=192\n",
        "  , use_batch_norm=True\n",
        "  , use_dropout=True)\n",
        "\n",
        "\n",
        "vae.compile(LEARNING_RATE, LOSS_FACTOR)\n",
        "vae.encoder.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_17\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 316, 316, 1) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_0 (Conv2D)         (None, 158, 158, 32) 320         encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 158, 158, 32) 128         encoder_conv_0[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_32 (LeakyReLU)      (None, 158, 158, 32) 0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_32 (Dropout)            (None, 158, 158, 32) 0           leaky_re_lu_32[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_1 (Conv2D)         (None, 79, 79, 64)   18496       dropout_32[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 79, 79, 64)   256         encoder_conv_1[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_33 (LeakyReLU)      (None, 79, 79, 64)   0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_33 (Dropout)            (None, 79, 79, 64)   0           leaky_re_lu_33[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_2 (Conv2D)         (None, 40, 40, 64)   36928       dropout_33[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 40, 40, 64)   256         encoder_conv_2[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_34 (LeakyReLU)      (None, 40, 40, 64)   0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_34 (Dropout)            (None, 40, 40, 64)   0           leaky_re_lu_34[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_3 (Conv2D)         (None, 20, 20, 64)   36928       dropout_34[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 20, 20, 64)   256         encoder_conv_3[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_35 (LeakyReLU)      (None, 20, 20, 64)   0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_35 (Dropout)            (None, 20, 20, 64)   0           leaky_re_lu_35[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "encoder_conv_4 (Conv2D)         (None, 10, 10, 128)  73856       dropout_35[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 10, 10, 128)  512         encoder_conv_4[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_36 (LeakyReLU)      (None, 10, 10, 128)  0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "dropout_36 (Dropout)            (None, 10, 10, 128)  0           leaky_re_lu_36[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "flatten_4 (Flatten)             (None, 12800)        0           dropout_36[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "mu (Dense)                      (None, 192)          2457792     flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "log_var (Dense)                 (None, 192)          2457792     flatten_4[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "encoder_output (Lambda)         (None, 192)          0           mu[0][0]                         \n",
            "                                                                 log_var[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 5,083,520\n",
            "Trainable params: 5,082,816\n",
            "Non-trainable params: 704\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    }
  ]
}